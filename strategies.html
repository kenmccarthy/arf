<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assessment Strategies | 2026 Assessment Redesign Framework</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Navigation Header -->
    <header class="navbar">
        <div class="container">
            <div class="nav-brand">
                <h1>2026 Assessment Redesign Framework</h1>
                <p class="subtitle">GenAI:N3 | Rethinking Assessment in the Age of Generative AI</p>
            </div>
            <nav class="nav-menu">
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="framework.html">Framework</a></li>
                    <li><a href="principles.html">Principles</a></li>
                    <li><a href="strategies.html" class="active">Strategies</a></li>
                    <li><a href="resources.html">Resources</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <!-- Page Header -->
    <div class="hero">
        <div class="container">
            <h2>Assessment Design Strategies</h2>
            <p>Practical approaches and design strategies for assessment in the age of GenAI</p>
        </div>
    </div>

    <!-- Main Content -->
    <main class="container">
        <!-- Breadcrumb -->
        <div class="breadcrumb">
            <a href="index.html">Home</a> â†’ <a href="framework.html">Framework</a> â†’ <span>Strategies</span>
        </div>

        <div class="layout-with-sidebar sidebar-left">
            <!-- Sidebar Navigation -->
            <aside class="sidebar">
                <h3>ðŸ“‘ On This Page</h3>
                <ul>
                    <li><a href="#risk-assessment">Risk Assessment</a></li>
                    <li><a href="#ai-assessment-scale">AI Assessment Scale</a></li>
                    <li><a href="#process-based">Process-Based Assessment</a></li>
                    <li><a href="#design-strategies">Design Strategies</a></li>
                    <li><a href="#quality-assurance">Quality Assurance</a></li>
                </ul>
            </aside>

            <!-- Content -->
            <div class="content">
            <section id="risk-assessment">
                <h2>Risk Assessment: Understanding Assessment Vulnerability</h2>

                <p>Different assessment types carry different levels of risk when it comes to inappropriate AI use. Understanding which assessments are most vulnerable helps you prioritise redesign efforts.</p>

                <h3>Assessment Risk Categories</h3>

                <div style="background-color: var(--bg-light); padding: 2rem; border-radius: 8px; margin: 2rem 0; border: 2px dashed var(--border-color); text-align: center;">
                    <p style="color: var(--text-light); font-style: italic; margin: 0;">
                        ðŸ“Š <strong>Risk Assessment Visual</strong><br>
                        <small>(The original PDF contained a visual diagram showing assessment types arranged by risk level)</small>
                    </p>
                </div>

                <table>
                    <thead>
                        <tr>
                            <th>Risk Level</th>
                            <th>Assessment Types</th>
                            <th>Why Vulnerable</th>
                            <th>Mitigation Strategy</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><span class="badge" style="background-color: #fee2e2; color: #991b1b; font-weight: bold;">HIGH</span></td>
                            <td>Essays, Unsupervised exams, Online quizzes, Reflective writing</td>
                            <td>Can be completed entirely by AI; final product hard to verify as original</td>
                            <td>Add oral defence, staged submissions, peer feedback, process visibility</td>
                        </tr>
                        <tr>
                            <td><span class="badge" style="background-color: #fef3c7; color: #92400e; font-weight: bold;">MEDIUM</span></td>
                            <td>Online quizzes (supervised), Research papers, Lab reports, Creative work</td>
                            <td>AI can assist; requires some independent judgment; verifiable in part</td>
                            <td>Structured peer review, annotation requirements, process documentation</td>
                        </tr>
                        <tr>
                            <td><span class="badge" style="background-color: #dcfce7; color: #166534; font-weight: bold;">LOW</span></td>
                            <td>Problem sets with working shown, Group projects, Oral presentations, Supervised exams</td>
                            <td>Requires real-time performance or visible collaboration; hard to outsource</td>
                            <td>Monitor progress; ask clarifying questions; direct observation</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Key Points</h3>
                <ul>
                    <li><strong>Risk level is NOT about assessment quality</strong>â€”essays are valuable and necessary</li>
                    <li><strong>Risk level indicates where redesign will have most impact</strong></li>
                    <li><strong>High-risk assessments benefit most from added process visibility</strong></li>
                    <li><strong>Mix assessment types</strong>â€”combination of formats is more robust than single approach</li>
                </ul>
            </section>

            <section id="ai-assessment-scale">
                <h2>AI Integration: The Five-Level Scale</h2>

                <p>Rather than a binary "allow AI or not," consider a spectrum of how AI integration aligns with learning outcomes. This scale (derived from Liu & Bridgeman, University of Sydney) shows five levels of AI integration:</p>

                <div style="margin: 2rem 0;">
                    <div class="callout-box track1">
                        <h4><span class="badge blue">Level 1</span> No AI</h4>
                        <p><strong>Use when:</strong> Assessing independent capability, foundational knowledge, professional standards requiring independent judgment</p>
                        <p><strong>Example:</strong> Clinical examination, mathematical problem exam, coding task assessed for independent understanding</p>
                    </div>

                    <div class="callout-box track1">
                        <h4><span class="badge blue">Level 2</span> Limited AI (for planning only)</h4>
                        <p><strong>Use when:</strong> Students can use AI for brainstorming, planning, but must execute independently</p>
                        <p><strong>Example:</strong> "You may use AI to brainstorm essay topics, but the essay itself must be your own writing"</p>
                    </div>

                    <div class="callout-box track2">
                        <h4><span class="badge green">Level 3</span> Structured AI (documented use)</h4>
                        <p><strong>Use when:</strong> AI use is integrated but must be transparent and documented</p>
                        <p><strong>Example:</strong> Annotated bibliography where students document: "I used ChatGPT to summarize this article. Here's what it said... Here's my critical evaluation..."</p>
                    </div>

                    <div class="callout-box track2">
                        <h4><span class="badge green">Level 4</span> Open AI (encouraged, with reflection)</h4>
                        <p><strong>Use when:</strong> AI is an integral part of the task and students develop AI literacy through use</p>
                        <p><strong>Example:</strong> "Analyse the quality of this AI-generated report. Identify strengths and limitations. Would you use this in professional practice? Why/why not?"</p>
                    </div>

                    <div class="callout-box">
                        <h4><span class="badge orange">Level 5</span> AI Exploration (investigation of AI itself)</h4>
                        <p><strong>Use when:</strong> The focus is explicitly on understanding, evaluating, or critiquing AI systems</p>
                        <p><strong>Example:</strong> "Prompt three different AI models with the same question and compare their responses. What are the implications?"</p>
                    </div>
                </div>

                <h3>Choosing Your Level</h3>
                <p>For each assessment, ask:</p>
                <ol>
                    <li><strong>What is the learning outcome?</strong> What do students need to demonstrate?</li>
                    <li><strong>Does AI use support or undermine this outcome?</strong></li>
                    <li><strong>What level of AI integration makes sense?</strong></li>
                    <li><strong>How will students know expectations?</strong> Be explicit in the brief.</li>
                </ol>
            </section>

            <section id="process-based">
                <h2>Making Learning Visible: Process-Based Assessment</h2>

                <p>One of the most effective ways to redesign assessment in the age of AI is to shift focus from <strong>final products</strong> to <strong>learning processes</strong>. When students must show how they arrived at conclusions, it becomes much harder to outsource thinking to AI.</p>

                <h3>Why Process Matters</h3>
                <ul>
                    <li><strong>Prevents outsourcing:</strong> The learning process is unique to each individual; harder to fake</li>
                    <li><strong>Improves learning:</strong> Students engage more deeply when they must articulate their thinking</li>
                    <li><strong>Provides assessment evidence:</strong> Makes understanding visible; easier to detect gaps</li>
                    <li><strong>Supports equity:</strong> Allows flexible timelines, feedback opportunities, revision cycles</li>
                    <li><strong>Builds confidence:</strong> Students see their own learning trajectory and growth</li>
                </ul>

                <h3>How to Make Learning Process Visible</h3>
                <ul>
                    <li><strong>Multi-stage tasks:</strong> Outline â†’ Draft â†’ Feedback â†’ Revision â†’ Final submission</li>
                    <li><strong>Annotated work:</strong> Students document decisions: "I chose this source because..." or "I changed my approach here because..."</li>
                    <li><strong>Reflective elements:</strong> Short reflections on thinking, challenges, revisions, and learning</li>
                    <li><strong>Oral components:</strong> Presentations, vivas, Q&A sessions where students explain and defend their work</li>
                    <li><strong>Collaborative elements:</strong> Group work with mechanisms to identify individual contributions</li>
                    <li><strong>Authentic problems:</strong> Real-world challenges requiring judgment and application</li>
                </ul>

                <div class="callout-box key-concept">
                    <strong>The Goal:</strong> Assessment that captures genuine understanding, judgment, and growthâ€”not just the final product.
                </div>
            </section>

            <section id="design-strategies">
                <h2>Practical Design Strategies</h2>

                <h3>1. Diversity of Assessment Methods</h3>
                <p>Use varied assessment methods across your programme and within modules:</p>
                <ul>
                    <li>Written assignments with oral components</li>
                    <li>Group projects with peer and tutor assessment</li>
                    <li>Practical demonstrations or performances</li>
                    <li>Portfolio-based assessment with reflections</li>
                    <li>Peer feedback and collaborative review</li>
                    <li>Problem-solving in real or simulated contexts</li>
                </ul>

                <h3>2. Structured Supervision and Timing</h3>
                <p>Use time and supervision strategically:</p>
                <ul>
                    <li><strong>Timed components:</strong> In-class writing, exams under supervision</li>
                    <li><strong>Live interaction:</strong> Oral presentations, vivas, discussions</li>
                    <li><strong>Staged deadlines:</strong> Outline due, then draft, then finalâ€”spreads work and allows feedback</li>
                    <li><strong>Checkpoints:</strong> Progress checks to ensure ongoing engagement</li>
                </ul>

                <h3>3. Authentic Assessment</h3>
                <p>Design assessments around real-world applications:</p>
                <ul>
                    <li>Tasks that professionals in the field actually do</li>
                    <li>Case studies from real practice</li>
                    <li>Collaboration with external stakeholders</li>
                    <li>Problems without single "right answer"</li>
                    <li>Constraints that mirror professional contexts</li>
                </ul>

                <h3>4. Feedback and Iteration</h3>
                <p>Build in opportunities for students to learn from feedback:</p>
                <ul>
                    <li>Peer feedback on drafts (structured with rubrics)</li>
                    <li>Tutor comments that guide improvement, not just judge quality</li>
                    <li>Opportunities to revise based on feedback (for some assessments)</li>
                    <li>Clear rubrics so students understand what success looks like</li>
                </ul>

                <h3>5. Integration of AI Literacy</h3>
                <p>Explicitly teach and assess AI literacy where appropriate:</p>
                <ul>
                    <li>Ask students to evaluate AI outputs critically</li>
                    <li>Require transparency about what tools were used and why</li>
                    <li>Design tasks that require judgment about when/whether to use AI</li>
                    <li>Reflect on how AI use affected the learning process</li>
                </ul>

                <h3>6. Programme-Level Coherence</h3>
                <p>Ensure consistency across modules:</p>
                <ul>
                    <li>Agree on terminology (what counts as "independent work"?)</li>
                    <li>Balance high-risk and low-risk assessments across semester</li>
                    <li>Mix assessment types (essays AND presentations AND projects)</li>
                    <li>Communicate consistent expectations about AI use to students</li>
                </ul>
            </section>

            <section id="quality-assurance">
                <h2>Quality Assurance at Programme Level</h2>

                <p>Assessment redesign can't be addressed solely at the module level. To ensure fairness, coherence, and academic standards, assessment strategy must be considered holistically at programme level.</p>

                <h3>Programme Teams Should:</h3>
                <ul>
                    <li><strong>Map assessments across all modules</strong> to ensure a balanced mix and varied methods</li>
                    <li><strong>Avoid clustering</strong> high-stakes assessments at the same time</li>
                    <li><strong>Communicate consistently</strong> about AI use expectations across modules</li>
                    <li><strong>Align with learning outcomes</strong> to ensure assessments measure intended learning</li>
                    <li><strong>Review equity</strong> to identify whether redesigns might disadvantage certain groups</li>
                    <li><strong>Integrate into quality processes</strong> (programme review, external examiner discussions)</li>
                </ul>

                <h3>Key Questions for Programme Teams</h3>
                <ul>
                    <li>What percentage of assessments are high-risk? Are we addressing them?</li>
                    <li>Do students receive consistent messaging about AI use expectations?</li>
                    <li>Are there assessments that could be more valid or equitable?</li>
                    <li>Does assessment design develop AI literacy explicitly?</li>
                    <li>Could any student group be disadvantaged by our assessment approaches?</li>
                    <li>Are external examiners aware of and aligned with our approach?</li>
                </ul>

                <div class="callout-box key-concept">
                    <strong>Programme-level coordination:</strong> Reduces burden on individual lecturers, enables shared resources, and ensures coherent student experience.
                </div>
            </section>

            <!-- Next Steps -->
            <section>
                <h2>Next Steps</h2>
                <p>Ready to implement these strategies?</p>
                
                <div class="grid-3">
                    <div class="step-card">
                        <h3>Review Your Assessments</h3>
                        <p>Use the risk assessment table to identify which of your current assessments need redesign</p>
                        <a href="resources.html" class="btn btn-primary" style="width: 100%; margin-top: 1rem; display: block; text-align: center;">Tools & Templates â†’</a>
                    </div>
                    <div class="step-card">
                        <h3>Apply the Principles</h3>
                        <p>Return to the core principles to guide your design decisions</p>
                        <a href="principles.html" class="btn btn-primary" style="width: 100%; margin-top: 1rem; display: block; text-align: center;">Review Principles â†’</a>
                    </div>
                    <div class="step-card">
                        <h3>Access Resources</h3>
                        <p>Find implementation guidance and templates</p>
                        <a href="resources.html" class="btn btn-primary" style="width: 100%; margin-top: 1rem; display: block; text-align: center;">View Resources â†’</a>
                    </div>
                </div>
            </section>
            </div>
        </div>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p><strong>2026 Assessment Redesign Framework</strong></p>
            <p>Developed by Dr Hazel Farrell as part of the N-TUTORR national project GenAI:N3</p>
            <p>This framework is a living resource that continues to evolve. Approaches outlined here can be adapted and expanded in response to disciplinary needs, emerging research, and evolving technologies.</p>
            <hr>
            <p class="footer-nav">
                <a href="index.html">Home</a> | 
                <a href="framework.html">Framework</a> | 
                <a href="principles.html">Principles</a> | 
                <a href="strategies.html">Strategies</a> | 
                <a href="resources.html">Resources</a>
            </p>
        </div>
    </footer>
<script data-goatcounter="https://arf.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
</body>
</html>
